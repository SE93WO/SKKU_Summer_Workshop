{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec74660e-e184-449f-be0e-c5cf1a22b759",
   "metadata": {},
   "source": [
    "## Load Food-101 dataset\n",
    "\n",
    "Start by loading a the Food-101 dataset from the huggingface Datasets library.\n",
    "\n",
    "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f8e31a-e23f-4bb1-bc51-8283f735852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary function to load datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the \"food101\" dataset, selecting only the training split and limiting to the first 10,000 samples\n",
    "food = load_dataset(\"food101\", split=\"train[:10000]\")\n",
    "\n",
    "# Split the loaded dataset into training and test sets, with 20% of the data allocated to the test set\n",
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a4985-0682-4838-9546-bec20e456483",
   "metadata": {},
   "source": [
    "Each example in the dataset has two fields:\r\n",
    "\r\n",
    "- `image`: a PIL image of the food item\r\n",
    "- `label`: the label class of the food item\r\n",
    "\r\n",
    "To make it easier for the model to get the label name from the label id, create a dictionary that maps the label name\r\n",
    "to an integer and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d95f5a9-3777-43cc-bbfa-855150eadd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of label names from the 'train' subset of the 'food' dataset\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Initialize two empty dictionaries for mapping labels to IDs and IDs to labels\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "# Iterate over the list of labels with their corresponding index\n",
    "for i, label in enumerate(labels):\n",
    "    # Populate the label2id dictionary with label as key and index (converted to string) as value\n",
    "    label2id[label] = str(i)\n",
    "    # Populate the id2label dictionary with index (converted to string) as key and label as value\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dc7b7-73b6-4f46-bd2e-c21e2f758121",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e1823-8772-4cd7-9964-d6e136b02b44",
   "metadata": {},
   "source": [
    "The next step is to load a ViT image processor to process the image into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9614412f-f5f8-44bd-9e33-ca66be45f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AutoImageProcessor class from the transformers library\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Specify the checkpoint for the pre-trained model to be used\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Load the image processor for the specified checkpoint\n",
    "# The 'use_fast=True' argument is used to enable fast processing if available\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3423def3-07fc-4101-9ccf-040337bb715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary transformations from the torchvision.transforms module\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "# Create a Normalize transform using the mean and standard deviation from the image processor\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "# Define the target size for cropping, using the height and width from the image processor\n",
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "# Compose a series of transformations:\n",
    "# 1. Randomly resize and crop the image to the target size\n",
    "# 2. Convert the image to a tensor\n",
    "# 3. Normalize the image using the previously defined normalization parameters\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736d806-e296-4087-aba0-034f34dd29d1",
   "metadata": {},
   "source": [
    "Then create a preprocessing function to apply the transforms and return the `pixel_values` - the inputs to the model - of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be469541-44e4-4a3c-b26e-70f7f0ba3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    # Apply the composed transformations to each image in the batch\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    \n",
    "    # Remove the original 'image' entries from the examples\n",
    "    del examples[\"image\"]\n",
    "    \n",
    "    # Return the modified examples\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d0823-f81a-46db-b1a3-53e820807764",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [with_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.with_transform) method. The transforms are applied on the fly when you load an element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcdb33e-db1f-47ee-ab0e-4006ae1825bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transforms function to the 'food' dataset\n",
    "food = food.with_transform(transforms)\n",
    "\n",
    "# The 'with_transform' method applies the specified transformation function to the dataset.\n",
    "# This ensures that the 'transforms' function will be called on each batch of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139cea3-af09-481a-9139-cc8e802759e1",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in huggingface Transformers, the `DefaultDataCollator` does not apply additional preprocessing such as padding.\n",
    "\n",
    "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89979959-64c6-4d0c-83fa-e53f59b8faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DefaultDataCollator class from the transformers library\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# The DefaultDataCollator automatically handles the collation of data batches,\n",
    "# making it easy to batch data together during training or evaluation.\n",
    "# It takes care of padding sequences to the same length and converting them into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1df2d-4d9c-4d7b-a910-2fb615e9e25c",
   "metadata": {},
   "source": [
    "## Evaluate \n",
    "\n",
    "Including a metric during training is often helpful for evaluating your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59219114-f593-4271-a9ea-420586424551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate library\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fce67-d3a1-48cc-b110-240ab2afa3e5",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c6e4e5-ce08-43ce-be41-91589a0b6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to compute metrics from evaluation predictions\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the eval_pred tuple\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply the argmax function to the predictions to get the predicted class labels\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Compute and return the accuracy using the accuracy object\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d3834-390b-4174-8913-2427c95491cd",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a13b984-caf8-4de7-bad8-3eaf965fc83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from the transformers library\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load a pre-trained image classification model using the specified checkpoint\n",
    "# and configure it for the specific number of labels and label mappings\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,           # The pre-trained model checkpoint\n",
    "    num_labels=len(labels),  # The number of unique labels in the dataset\n",
    "    id2label=id2label,       # A dictionary mapping IDs to labels\n",
    "    label2id=label2id,       # A dictionary mapping labels to IDs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fda49-98b4-44d2-a104-2f788d9efc0b",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\r\n",
    "\r\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). It is important you don't remove unused columns because that'll drop the `image` column. Without the `image` column, you can't create `pixel_values`. Set `remove_unused_columns=False` to prevent this behavior! The only other required parameter is `output_dir` which specifies where to save your mode). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\r\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\r\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e82da0-296d-4bc9-95b6-0f73be3ff7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.conda/envs/huggingface/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Import the TrainingArguments and Trainer classes from the transformers library\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",                  # Directory to save the model checkpoints and logs\n",
    "    remove_unused_columns=False,         # Retain all columns in the dataset\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",               # Save the model at the end of each epoch\n",
    "    learning_rate=5e-5,                  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=128,     # Batch size for training\n",
    "    gradient_accumulation_steps=4,       # Number of steps to accumulate gradients before updating\n",
    "    per_device_eval_batch_size=128,      # Batch size for evaluation\n",
    "    num_train_epochs=3,                  # Number of training epochs\n",
    "    warmup_ratio=0.1,                    # Ratio of total training steps used for learning rate warmup\n",
    "    logging_steps=10,                    # Log training progress every 10 steps\n",
    "    load_best_model_at_end=True,         # Load the best model found during training at the end\n",
    "    metric_for_best_model=\"accuracy\",    # Metric to determine the best model\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                          # The model to train\n",
    "    args=training_args,                   # The training arguments defined above\n",
    "    data_collator=data_collator,          # Data collator for batching\n",
    "    train_dataset=food[\"train\"],          # Training dataset\n",
    "    eval_dataset=food[\"test\"],            # Evaluation dataset\n",
    "    tokenizer=image_processor,            # Tokenizer (image processor in this case)\n",
    "    compute_metrics=compute_metrics,      # Function to compute metrics during evaluation\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7d0419-e01c-4bc9-b531-5f6c19264698",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The simplest way to try out finetuend model for inference is to use it in a pipeline(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d2294-1eba-4186-b6ca-efa9c56baa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a small validation split of the \"food101\" dataset\n",
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "\n",
    "# Extract the first image from the validation dataset\n",
    "image = ds[\"image\"][0]\n",
    "\n",
    "# Create an image classification pipeline using the model checkpoint\n",
    "classifier = pipeline(\"image-classification\", model=\"model/checkpoint-45\")\n",
    "\n",
    "# Use the classifier pipeline to predict the class of the extracted image\n",
    "predictions = classifier(image)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1536d1e-7d5a-4a43-9a5e-40dbb0eb35b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
